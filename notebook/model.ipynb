{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'belongs_to_collection', 'budget', 'genres', 'homepage',\n",
       "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
       "       'popularity', 'production_companies', 'production_countries',\n",
       "       'release_date', 'runtime', 'spoken_languages', 'status', 'tagline',\n",
       "       'title', 'Keywords', 'cast', 'crew', 'revenue', 'collection_name',\n",
       "       'has_collection', 'Action', 'Adventure', 'Animation', 'Comedy', 'Crime',\n",
       "       'Documentary', 'Drama', 'Family', 'Fantasy', 'Foreign', 'History',\n",
       "       'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie',\n",
       "       'Thriller', 'War', 'Western', 'is_english', 'number_of_languages',\n",
       "       'keyword_text', 'number_of_keywords', 'cast_size', 'crew_size',\n",
       "       'has_homepage', 'budget_log', 'popularity_log', 'release_date_year',\n",
       "       'release_date_weekday', 'release_date_month', 'release_date_weekofyear',\n",
       "       'release_date_day', 'release_date_quarter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['budget','runtime','has_collection', 'Action', 'Adventure', 'Animation', 'Comedy', 'Crime',\n",
    "       'Documentary', 'Drama', 'Family', 'Fantasy', 'Foreign', 'History',\n",
    "       'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie',\n",
    "       'Thriller', 'War', 'Western', 'is_english', 'number_of_languages', 'number_of_keywords', 'cast_size', 'crew_size',\n",
    "       'has_homepage', 'budget_log', 'popularity_log','revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['has_collection'] = data['has_collection'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = data[data['budget'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we need to scale the value for \n",
    "    - budget\n",
    "    - runtime\n",
    "    - number_of_languages\n",
    "    - number_of_keywords\n",
    "    - cast_size \n",
    "    - crew_size\n",
    "    - budget_log \n",
    "    - popularity_log\n",
    "    and other values are binary values so, binary feature we don't need to scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_binary_feature = ['budget','runtime', 'number_of_keywords',\n",
    "                      'cast_size', 'crew_size','has_homepage', 'budget_log', 'popularity_log']\n",
    "binary_feature = ['number_of_languages','has_collection', 'Action', 'Adventure', 'Animation', 'Comedy', 'Crime',\n",
    "                  'Documentary', 'Drama', 'Family', 'Fantasy', 'Foreign', 'History',\n",
    "                  'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie',\n",
    "                  'Thriller', 'War', 'Western', 'is_english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = data[non_binary_feature].values\n",
    "feature_2 = data[binary_feature].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.concatenate((feature_1,feature_2),axis=1)\n",
    "label = np.log1p(data.revenue.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature, label,random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.2776351  18.73135364 21.19109817 19.8370173  15.78764338 17.88428864\n",
      " 18.01789614 18.54923087 16.76439857 18.19831119]\n"
     ]
    }
   ],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(X_train,y_train)\n",
    "\n",
    "pred_linner = linear.predict(X_test)\n",
    "print(pred_linner[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.451557704210658"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, pred_linner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030942555390532304"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_log_error(y_test,pred_linner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.35279163 14.85232632 13.4544283  17.49679307 15.1431466  22.59744295\n",
      " 16.91196859 13.18847791 17.02895059 18.44335373]\n"
     ]
    }
   ],
   "source": [
    "pred_linner = linear.predict(X_test)\n",
    "print(pred_linner[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here pred values are negative so we can't use mean_squared_log_error, we are using mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.207282162232285"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, pred_linner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04344660960734203"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_log_error(y_test,pred_linner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(C=10,kernel='linear')\n",
    "parameters = {'kernel':['linear', 'poly', 'rbf'],\n",
    "             'degree':[2,3],\n",
    "             'C':[1,10,20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(model, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n",
    "pred_svm = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msle = mean_squared_log_error(y_test,pred_svm)\n",
    "mse = mean_squared_error(y_test,pred_svm)\n",
    "print(f\"MSLE is : {msle}\\nMSE is : {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updated_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "parameters = {'n_estimators':[10,20,30,40,50],\n",
    "              'min_samples_leaf' : [5, 8,10],\n",
    "              'max_depth' :[2,3,4,5]\n",
    "         }\n",
    "rf= GridSearchCV(model,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)\n",
    "pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSLE is : 0.03212433660681026\n",
      "MSE is : 3.598188776533107\n"
     ]
    }
   ],
   "source": [
    "msle = mean_squared_log_error(y_test,pred_rf)\n",
    "mse = mean_squared_error(y_test,pred_rf)\n",
    "print(f\"MSLE is : {msle}\\nMSE is : {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "parameters = {'n_estimators':[10,20,30,40,50],\n",
    "              'min_samples_leaf' : [5, 8,10],\n",
    "              'max_depth' :[2,3,4,5]\n",
    "         }\n",
    "rf= GridSearchCV(model,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)\n",
    "pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSLE is : 0.04124708459114953\n",
      "MSE is : 4.852466563970675\n"
     ]
    }
   ],
   "source": [
    "msle = mean_squared_log_error(y_test,pred_rf)\n",
    "mse = mean_squared_error(y_test,pred_rf)\n",
    "print(f\"MSLE is : {msle}\\nMSE is : {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSLE is : 0.03212433660681026\n",
      "MSE is : 3.598188776533107\n"
     ]
    }
   ],
   "source": [
    "msle = mean_squared_log_error(y_test,pred_rf)\n",
    "mse = mean_squared_error(y_test,pred_rf)\n",
    "print(f\"MSLE is : {msle}\\nMSE is : {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tea pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_config_dict = {\n",
    "\n",
    "    'sklearn.ensemble.ExtraTreesRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.ensemble.GradientBoostingRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'loss': [\"ls\", \"lad\", \"huber\", \"quantile\"],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "    },\n",
    "\n",
    "    'sklearn.ensemble.AdaBoostRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'loss': [\"linear\", \"square\", \"exponential\"]\n",
    "    },\n",
    "\n",
    "    'sklearn.tree.DecisionTreeRegressor': {\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21)\n",
    "    },\n",
    "\n",
    "    'sklearn.neighbors.KNeighborsRegressor': {\n",
    "        'n_neighbors': range(1, 101),\n",
    "        'weights': [\"uniform\", \"distance\"],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "\n",
    "    'sklearn.linear_model.LassoLarsCV': {\n",
    "        'normalize': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.svm.LinearSVR': {\n",
    "        'loss': [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "        'dual': [True, False],\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "        'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1.]\n",
    "    },\n",
    "\n",
    "    'sklearn.ensemble.RandomForestRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'nthread': [1],\n",
    "        'objective': ['reg:squarederror']\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TPOTRegressor(generations=50, population_size=20, config_dict= regressor_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSLE is : 0.03901428074676565\n",
      "MSE is : 4.61548006804557\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "msle = mean_squared_log_error(y_test,pred)\n",
    "mse = mean_squared_error(y_test,pred)\n",
    "print(f\"MSLE is : {msle}\\nMSE is : {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export('test_logirized_2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(df):\n",
    "    global json_cols\n",
    "    global train_dict\n",
    "\n",
    "    df[['release_month','release_day','release_year']]=df['release_date'].str.split('/',expand=True).replace(np.nan, 0).astype(int)\n",
    "    df['release_year'] = df['release_year']\n",
    "    df.loc[ (df['release_year'] <= 19) & (df['release_year'] < 100), \"release_year\"] += 2000\n",
    "    df.loc[ (df['release_year'] > 19)  & (df['release_year'] < 100), \"release_year\"] += 1900\n",
    "    \n",
    "    releaseDate = pd.to_datetime(df['release_date']) \n",
    "    df['release_dayofweek'] = releaseDate.dt.dayofweek \n",
    "    df['release_quarter'] = releaseDate.dt.quarter     \n",
    "    \n",
    "    df['originalBudget'] = df['budget']\n",
    "    df['inflationBudget'] = df['budget'] + df['budget']*1.8/100*(2018-df['release_year']) #Inflation simple formula\n",
    "    df['budget'] = np.log1p(df['budget']) \n",
    "    \n",
    "    \n",
    "    # Thanks to this Kernel for the next 7 features https://www.kaggle.com/artgor/eda-feature-engineering-and-model-interpretation\n",
    "    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n",
    "    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n",
    "    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n",
    "    df['belongs_to_collection'] = [literal_eval(i) if not pd.isnull(i) else i for i in df['belongs_to_collection'] ]\n",
    "    \n",
    "    #df['_collection_name'] = df['belongs_to_collection'].apply(lambda x: x[0]['name'] if not pd.isnull(x) else x)\n",
    "    df['_collection_name'] = [i[0]['name'] if not pd.isnull(i) else i for i in df['belongs_to_collection']]\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(df['_collection_name'].fillna('')))\n",
    "    df['_collection_name'] = le.transform(df['_collection_name'].fillna('').astype(str))\n",
    "    df['_num_Keywords'] = df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    df['_num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
    "\n",
    "    \n",
    "    \n",
    "    df['_popularity_mean_year'] = df['popularity'] / df.groupby(\"release_year\")[\"popularity\"].transform('mean')\n",
    "    df['_budget_runtime_ratio'] = df['budget']/df['runtime'] \n",
    "    df['_budget_popularity_ratio'] = df['budget']/df['popularity']\n",
    "    df['_budget_year_ratio'] = df['budget']/(df['release_year']*df['release_year'])\n",
    "    df['_releaseYear_popularity_ratio'] = df['release_year']/df['popularity']\n",
    "    df['_releaseYear_popularity_ratio2'] = df['popularity']/df['release_year']\n",
    "\n",
    "    #df['_popularity_totalVotes_ratio'] = df['totalVotes']/df['popularity']\n",
    "    #df['_rating_popularity_ratio'] = df['rating']/df['popularity']\n",
    "    #df['_rating_totalVotes_ratio'] = df['totalVotes']/df['rating']\n",
    "    #df['_totalVotes_releaseYear_ratio'] = df['totalVotes']/df['release_year']\n",
    "    #df['_budget_rating_ratio'] = df['budget']/df['rating']\n",
    "    #df['_runtime_rating_ratio'] = df['runtime']/df['rating']\n",
    "    #df['_budget_totalVotes_ratio'] = df['budget']/df['totalVotes']\n",
    "    \n",
    "    df['has_homepage'] = 1\n",
    "    df.loc[pd.isnull(df['homepage']) ,\"has_homepage\"] = 0\n",
    "    \n",
    "    df['isbelongs_to_collectionNA'] = 0\n",
    "    df.loc[pd.isnull(df['belongs_to_collection']) ,\"isbelongs_to_collectionNA\"] = 1\n",
    "    \n",
    "    df['isTaglineNA'] = 0\n",
    "    df.loc[df['tagline'] == 0 ,\"isTaglineNA\"] = 1 \n",
    "\n",
    "    df['isOriginalLanguageEng'] = 0 \n",
    "    df.loc[ df['original_language'] == \"en\" ,\"isOriginalLanguageEng\"] = 1\n",
    "    \n",
    "    df['isTitleDifferent'] = 1\n",
    "    df.loc[ df['original_title'] == df['title'] ,\"isTitleDifferent\"] = 0 \n",
    "\n",
    "    df['isMovieReleased'] = 1\n",
    "    df.loc[ df['status'] != \"Released\" ,\"isMovieReleased\"] = 0 \n",
    "\n",
    "    # get collection id\n",
    "    #df['collection_id'] = df['belongs_to_collection'].apply(lambda x : np.nan if len(x)==0 else x[0]['id'])\n",
    "    df['collection_id'] = [i[0]['id'] if not pd.isnull(i) else i for i in df['belongs_to_collection']]\n",
    "    df['original_title_letter_count'] = df['original_title'].str.len() \n",
    "    df['original_title_word_count'] = df['original_title'].str.split().str.len() \n",
    "\n",
    "\n",
    "    df['title_word_count'] = df['title'].str.split().str.len()\n",
    "    df['overview_word_count'] = df['overview'].str.split().str.len()\n",
    "    df['tagline_word_count'] = df['tagline'].str.split().str.len()\n",
    "    \n",
    "    df['production_countries_count'] = df['production_countries'].apply(lambda x : len(x))\n",
    "    df['production_companies_count'] = df['production_companies'].apply(lambda x : len(x))\n",
    "    df['cast_count'] = df['cast'].apply(lambda x : len(x))\n",
    "    df['crew_count'] = df['crew'].apply(lambda x : len(x))\n",
    "    \n",
    "\n",
    "    df['meanPopularityByYear'] = df.groupby(\"release_year\")[\"popularity\"].aggregate('mean')\n",
    "    df['meanBudgetByYear'] = df.groupby(\"release_year\")[\"budget\"].aggregate('mean')\n",
    "    df['medianBudgetByYear'] = df.groupby(\"release_year\")[\"budget\"].aggregate('median')\n",
    "\n",
    "    for col in ['genres', 'production_countries', 'spoken_languages', 'production_companies'] :\n",
    "        df[col] = df[col].map(lambda x: sorted(list(set([n if n in train_dict[col] else col+'_etc' for n in [d['name'] for d in x]])))).map(lambda x: ','.join(map(str, x)))\n",
    "        temp = df[col].str.get_dummies(sep=',')\n",
    "        df = pd.concat([df, temp], axis=1, sort=False)\n",
    "    df.drop(['genres_etc'], axis = 1, inplace = True)\n",
    "    \n",
    "    df = df.drop(['id','belongs_to_collection','genres','homepage','imdb_id','overview','runtime'\n",
    "    ,'poster_path','production_companies','production_countries','release_date','spoken_languages'\n",
    "    ,'status','title','Keywords','cast','crew','original_language','original_title','tagline', 'collection_id'\n",
    "    ],axis=1)\n",
    "    \n",
    "    df.fillna(value=0.0, inplace = True) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.visualization import dealing_null_value\n",
    "\n",
    "data = pd.read_csv('../data/ML 1.csv')\n",
    "train = data.iloc[:2100]\n",
    "test = data.iloc[2100:]\n",
    "\n",
    "for i in json_cols:\n",
    "    train = dealing_null_value(train,i)\n",
    "    test = dealing_null_value(test,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dictionary(s):\n",
    "    try:\n",
    "        d = eval(s)\n",
    "    except:\n",
    "        d = {}\n",
    "    return d\n",
    "    \n",
    "def get_json_dict(df) :\n",
    "    global json_cols\n",
    "    result = dict()\n",
    "    for e_col in json_cols :\n",
    "        d = dict()\n",
    "        rows = df[e_col].values\n",
    "        for row in rows :\n",
    "            if row is None : continue\n",
    "            for i in row :\n",
    "                if i['name'] not in d :\n",
    "                    d[i['name']] = 0\n",
    "                d[i['name']] += 1\n",
    "        result[e_col] = d\n",
    "    return result\n",
    "\n",
    "train_dict = get_json_dict(train)\n",
    "test_dict = get_json_dict(test)\n",
    "\n",
    "# remove cateogry with bias and low frequency\n",
    "for col in json_cols :\n",
    "    \n",
    "    remove = []\n",
    "    train_id = set(list(train_dict[col].keys()))\n",
    "    test_id = set(list(test_dict[col].keys()))   \n",
    "    \n",
    "    remove += list(train_id - test_id) + list(test_id - train_id)\n",
    "    for i in train_id.union(test_id) - set(remove) :\n",
    "        if train_dict[col][i] < 10 or i == '' :\n",
    "            remove += [i]\n",
    "            \n",
    "    for i in remove :\n",
    "        if i in train_dict[col] :\n",
    "            del train_dict[col][i]\n",
    "        if i in test_dict[col] :\n",
    "            del test_dict[col][i]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = prepare(pd.concat([train, test]).reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.fillna(0)\n",
    "all_data = all_data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [i for i in (list(all_data.columns)) if i != 'revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['revenue_updated'] = np.log1p(all_data['revenue'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data[feature_list], all_data[['revenue_updated']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "linner = xgb.XGBRegressor()\n",
    "parameters = {'max_depth':[2,3,4,5,6], \n",
    "              'learning_rate':[0.1, 0.2, 0.15],\n",
    "              'n_estimators':[10,15,18,20,25,30]}\n",
    "clf_xgb = GridSearchCV(linner, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5, 6], 'learning_rate': [0.1, 0.2, 0.15], 'n_estimators': [10, 15, 18, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_value = clf_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8243978"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029023834"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_log_error(y_test, pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.2, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_r = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
    "       learning_rate=0.2, max_delta_step=0, max_depth=2,\n",
    "       min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
    "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "       subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.2, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_r.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = {}\n",
    "for col, score in zip(X_train.columns, xgb_r.feature_importances_):\n",
    "    if score > 0.0:\n",
    "        important_columns[col] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sorted(important_columns.items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict([('_num_cast', 0.018029619),\n",
    " ('budget', 0.019826835),\n",
    " ('_collection_name', 0.022449156),\n",
    " ('release_year', 0.025995579),\n",
    " ('genders_2_crew', 0.04345532),\n",
    " ('_releaseYear_popularity_ratio', 0.058784723),\n",
    " ('_popularity_mean_year', 0.07657793),\n",
    " ('popularity', 0.11947978),\n",
    " ('_budget_year_ratio', 0.13055435),\n",
    " ('inflationBudget', 0.2958483)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feature= list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5, 6], 'learning_rate': [0.1, 0.2, 0.15], 'n_estimators': [10, 15, 18, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linner = xgb.XGBRegressor()\n",
    "parameters = {'max_depth':[2,3,4,5,6], \n",
    "              'learning_rate':[0.1, 0.2, 0.15],\n",
    "              'n_estimators':[10,15,18,20,25,30]}\n",
    "clf_xgb1 = GridSearchCV(linner, parameters)\n",
    "clf_xgb1.fit(X_train[important_feature], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf_xgb1.predict(X_test[important_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7610962"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0286957"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_log_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
